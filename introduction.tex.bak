\chapter{Introduction}\label{intro}
The frequency of data access at the users' end has been increased by a large number for the past few years. To ensure low latency at the users' end, it is preferable to reside the data as close to the users as possible. Moreover, in present world data privacy has been a great concern and almost all the nations require the data of their citizens not to reside in some place across their national border. Now its a big concern to maintain privacy regulations and keep foreign countries from being able to subpoena data. To mitigate these two issues, almost all the companies have been building their data centers all around the world. As a result, the sparsity of data has been increased by a huge amount during the last few years.

On the other hand, the amount of data generating in present world is quite large. This huge volume has introduced  a new term ``Big Data". In general ``Big Data" is nothing but the large volume of data that can be both structured and unstructured. The excessive use of social media, scientific instruments, portable devices, sensors are the main reason behind the generation of big data. Data analysis, capture, curation, search, share, storage, transfer, visualization, query, updating, information privacy everything associated with big data has been the challenges of the present world. In case of big data the can be both wide and tall that means the dimension of the data can be quite large. As a result available techniques might not be able to perform the data analysis with the provided limited hardware. Therefore, we need  approaches that have the property of scalability.

For researchers, however, big data brings new sets of challenges like how to efficiently and effectively handle big data in commodity computers, how to apply conventional
algorithms, how
to properly manage big data in distributed setting etc. Specially, in field of
Machine Leaning and
Pattern Recognition, big data poses a threat, because the conventional algorithms
were designed
solely to fulfil its purpose where entire dataset can fit in memory. Distributed
setting was not
taken into consideration. In a nutshell, to extract any meaningful insight from this
big data, we
need powerful tools to analyse it, elastic frameworks to cope up with its sheer
volume and most
importantly we have to rethink and redesign existing algorithms which are most
suitable for
smaller sized dataset and do not take advantage of clusters.
In this thesis, we consider one kind of the machine learning algorithms, namely Probabilistic Principal Component Analysis(PPCA) and try to make it practically usable in analysing tall and wide big data which are distributed throughout the whole world. In this chapter we provide the motivation behind our research in Section \ref{sec:1}, we discuss the advantages of using PPCA in Big Data Analysis in Section \ref{sec:2}, in Section \ref{sec:3} we present our contribution in data analysis field and in Section \ref{sec:4} we briefly describe the organization of this book. 

\section{Motivation}
	\label{sec:1}
Let us assume some scenario. Let us assume that We have good availability of  machine learning models to do various kinds of data analytics. Moreoevr, our data is of the size $10000 \times 200$. That means it contains $10000$ sample which each sample has $200$ features. The machine learning models will be capable to fit such data pretty easily. As soon as the data usage and data generation rate increases the data size gets larger to $200000 \times 2000$. This time the machine learning models fails to fit such higher dimensional data. Fortunately, we have Principal Component Analysis (PCA) technique to reduce the dimensionality of the data. Thus we can get $100$ principal components from the data. Now the data is compressed enough to fit in our Machine learning models. But if the dimension of data become more higher like in millions or billions then such Tall and Wide data will neither be suitable to fit in machine learning models nor the tradition PCA algorithms can handle such large dimensional data. More question arise if the data get to be geographically distributed. We were intended to give such a method that can answer all these questions and hence solve the problems of geographically distributed higher dimensional Tall and Wide Data.

\section{Probabilistic PCA (PPCA) on Big Data}
	\label{sec:2}
As PPCA incorporates both probabilistic model of data and EM algorithm for deriving the parameters, it has some advantages from from both sides. Here we elaborate the major ones:
\begin{itemize}
	\item Firstly and most importantly, EM algorithm for deriving the parameter is computationally efficient, if only few principal components are required. If we want only $k$
	components, where $k << D$ and size of data is $N \times D$, time complexity of PPCA is $O(NDk)$. It does not require to calculate a big dense data covariance matrix as intermediate data, so communication complexity is also less. Calculating data covariance matrix
	is costly in terms of both computational and communication complexity. For this reason.
	deterministic PCA methods (like EVD, SVD) do not scale well with data (time complexity: $O(ND^2))$. More detailed analysis can be found in the technical report \cite{bishop}. Thus
	EM steps in PPCA give a well scalable algorithm and is a great advantage for big data
	analysis.
	\item For large dataset, it is very common that some entries of data can be missing. As big data
	can have missing values, deterministic PCA algorithm will give erroneous result and will
	not be suitable. But for PPCA it is not a problem. Since iterative method of PPCA uses
	expectation maximization, projected data or expected values of latent variable can still
	be derived even if some values are missing in random. This benefit of handling missing
	values is one of the major advantages of PPCA.
	\item Deterministic PCA methods are not aware of the isotropic noise that is considered by
	PPCA. It is because these methods consider the squared distance of the new data points
	from their projections into the principal subspace. So, even for data points that are ran-
	domly far from the training data, conventional PCA will give a low reconstruction cost
	if these points are close to the principal subspace. We do not face similar problems in
	PPCA as it takes ‘noise’ as a parameter and assigns likelihood accordingly.
	
	\item For a well defined problem, target dimension k can be known beforehand. But in many
	
	data reduction problems the optimal It may not be known in advance. Thus we need such
	
	a method that can find dimensionality of the principal subspace automatically from the
	
	data. This can be done by a Bayesian treatment of PCA and PPCA forms the basis for it.
	
	This is a great advantage for unsupervised learning.
	
	\item As we are deriving parameters of a probabilistic model of data, we can easily generate
	`synthesized' or `fantasy data from the distribution.
	
	\item We can combine multiple PPCA models as a probabilistic mixture for complex models to get better accuracy. Using EM algorithm we can easily train mixtures of PPCA models.
	
	\item Number of independent parameters in PPCA model grows only linearly with dimension of data D for a fixed target dimension k. Because of this reason, number of independent parameters for multivariate Gaussian distribution can be restricted while still allowing if to capture the dominant correlation in data.
	
	\item Lastly, PPCA can be applied to classification problems because we can use it to model class-conditional densities.
	\end{itemize}

\newpage
\section{Contribution}
	\label{sec:3}
Summarizing, our main contributions are:
\begin{itemize}
\item We propose an approach of PPCA that deals with geographically distributed Tall and Wide Big Data and provide an in-depth study of the relative merits against recent efficient approach \textit{sPCA} \cite{elgamal}.
\item We propose a system that builds upon Apache Spark  \cite{spark} environment and extends their functionality to support multiple data clusters learning applications.
\item We propose an apparently time efficient method to accumulate partially generated results in various data clusters.
\item Our proposed approach  will minimize the number of transfers among data centers thus minimize the inter data center bandwidth utilization.
\item Our method will be scalable to the available hardware of the data clusters. It will allow us to handle Tall and Wide big data in commodity computers even with memory of $6/8GB$.
\item We will present our experimental results implemented on the clusters in our undergraduate thesis laboratory that will justify the validity of our method.
\end{itemize}

\section{Organization of this Book}
	\label{sec:4}
	The rest of the book is organized as follows. In Chapter \ref{c:2}, we define some basic terminologies. In Chapter \ref{c:3}, we describe all the mathematical background characteristics of PCA. In Chapter \ref{c:4}, we write about our focus. In Chapter \ref{c:5}, we describe our preliminary works on regression analysis. In Chapter \ref{c:6}, we describe our main approach to solve the existing problem. Chapter \ref{c:7} describes the properties of our approach while Chapter \ref{c:8} gives a brief on our  Spark implementation. With showing the experimenta;l results in Chapter \ref{c:9}, the book concludes with Chapter \ref{c:10}. 

\endinput